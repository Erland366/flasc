gpu: "0"
dir: "runs"
name: "experiment"
save: "true"
dataset: "20newsgroups"
iid-alpha: 0.1
clients: 100
model: "gpt2"
seed: 0

# Evaluation settings
eval-freq: 10
eval-first: "false"
eval-frac: 1.0
eval-masked: "true"

# Server settings
server-opt: "yogi"
server-lr: 1.0e-3
server-batch: 10  # numbers of sampled clients per round
server-rounds: 100
server-freeze: "false"
server-hparams:
  beta1: 0.9
  beta2: 0.999
  tau: 1.0e-8

# Client settings
client-lr: 1.0e-3
client-batch: 16  # batch size for each client
client-epochs: 1
client-freeze: "false"

# LoRA settings
freeze-a: "false"
lora-rank: 16
lora-alpha: 16

# Logging settings
use_wandb: true
use_tensorboard: true
project_name: "federated_merging"
entity: null

# Merging strategy
merging_strategy: "fedavg"
merging_kwargs:
  
# Federated optimizer
fed_optimizer: "fedyogi"