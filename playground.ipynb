{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study the simple setting\n",
    "\n",
    "finetune two models and merge; can also do it in the FL setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xingyu/anaconda3/envs/flasc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-04 12:21:24.495088: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-04 12:21:24.514730: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from merging import MergingFactory\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import data_utils\n",
    "import torch\n",
    "from train_utils import get_metric\n",
    "from copy import deepcopy\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    dataset=\"20newsgroups\"\n",
    "    client_batch=16\n",
    "    clients=50  # \n",
    "    iid_alpha=0.1  # the larger, the more non-iid\n",
    "    seed=0\n",
    "    eval_frac=1.0\n",
    "    lora_rank=16\n",
    "    lora_alpha=16\n",
    "    freeze_a = \"false\"\n",
    "    merging_strategy = \"average\"\n",
    "    \n",
    "# set seed for debugging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "set_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare models and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2374656 parameters (1.91% of original 124455168)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xingyu/anaconda3/envs/flasc/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clients, valloader, testloader, test_batch = data_utils.build_dataset(args.dataset,\n",
    "                                                                      args.client_batch,\n",
    "                                                                      args.clients,\n",
    "                                                                      args.iid_alpha, args.seed, args.eval_frac)\n",
    "clients = [clients[0], clients[1]]  # we take out two loaders to use\n",
    "\n",
    "import models\n",
    "model = models.build_model(args.dataset)\n",
    "total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "models.add_adapters_dataset(args.dataset, model, args.lora_rank, args.lora_alpha)\n",
    "\n",
    "def str2bool(s):\n",
    "    return s.lower() == 'true'\n",
    "\n",
    "if str2bool(args.freeze_a):\n",
    "    for n,p in model.named_parameters():\n",
    "        if \"lora_A\" in n:\n",
    "            p.requires_grad = False\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Training {trainable} parameters ({100*trainable/total:.2f}% of original {total})\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_freeze = False\n",
    "server_lr = 1 # for the original fedavg, this is one; for others like fedadam, we can set it to 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_model = model\n",
    "orig_server_model = deepcopy(server_model)  # backup if need\n",
    "\n",
    "server_params = {n:p for n,p in server_model.named_parameters() if p.requires_grad}\n",
    "server_mask = {n:torch.ones_like(p) for n,p in server_params.items()}\n",
    "server_freeze = False\n",
    "if server_freeze:\n",
    "    for p in server_params.values():\n",
    "        p.requires_grad = False\n",
    "\n",
    "server_opt = torch.optim.SGD(server_params.values(), lr=server_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merging_strategy =  args.merging_strategy\n",
    "merger = MergingFactory.get_merging_strategy(merging_strategy, server_model, args=args)\n",
    "scaling_coefficient = 1.0  # no need to change\n",
    "\n",
    "eval_accu = 0\n",
    "def eval_loop(model, loader):\n",
    "    model.eval()\n",
    "    stats_acc = {}\n",
    "    for x,y in loader:\n",
    "        with torch.no_grad():\n",
    "            _, stats = test_batch(model, x, y)\n",
    "        for k,v in stats.items():\n",
    "            stats_acc[k] = stats_acc.get(k, 0) + v\n",
    "    stats_acc['loss'] /= stats_acc['count']\n",
    "    return stats_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_batch = 2  # how many clients we use\n",
    "client_lr = 1e-3\n",
    "\n",
    "clients = [clients[0], clients[1]]  # the client loaders\n",
    "client_epochs = 6  # local training epochs; in standard FL setting, this is 1\n",
    "l2_clip_norm = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell only finetunes two models, without merging. A simple FL setup can be recovered by increasing the global rounds and adding the merging code in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 0 | client 0, epoch 0 | loss 6.5722:   0%|          | 0/1 [00:00<?, ?it/s] We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
      "eval: 0 | client 0, epoch 1 | loss 1.0522:   0%|          | 0/1 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.5316455696202531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 0 | client 0, epoch 2 | loss 0.7329:   0%|          | 0/1 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.6455696202531646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 0 | client 0, epoch 3 | loss 0.6980:   0%|          | 0/1 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.890295358649789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 0 | client 0, epoch 4 | loss 0.0898:   0%|          | 0/1 [00:08<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9620253164556962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 0 | client 0, epoch 5 | loss 0.0607:   0%|          | 0/1 [00:10<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9915611814345991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 0 | client 0, epoch 5 | loss 0.0023:   0%|          | 0/1 [00:12<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 0 | client 1, epoch 1 | loss 0.7112:   0%|          | 0/1 [00:14<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.48255813953488375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 0 | client 1, epoch 2 | loss 0.7028:   0%|          | 0/1 [00:16<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.8197674418604651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 0 | client 1, epoch 3 | loss 0.1618:   0%|          | 0/1 [00:17<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.936046511627907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 0 | client 1, epoch 4 | loss 0.0761:   0%|          | 0/1 [00:19<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9883720930232558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 0 | client 1, epoch 5 | loss 0.0131:   0%|          | 0/1 [00:20<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9941860465116279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 0 | client 1, epoch 5 | loss 0.0016: 100%|██████████| 1/1 [00:21<00:00, 21.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rounds = 1\n",
    "\n",
    "pbar = tqdm(range(rounds))\n",
    "\n",
    "client_models = [deepcopy(server_model) for _ in range(2)]\n",
    "client_ids = [0, 1]\n",
    "# client_loaders=[clients[0]]\n",
    "client_loaders = clients\n",
    "\n",
    "\n",
    "for round_idx, rnd in enumerate(pbar):\n",
    "    neg_client_deltas = []\n",
    "    stats_acc = {}\n",
    "    sample_num_list = torch.Tensor([len(client_loader) for client_loader in client_loaders])  # record the number of batches for each client\n",
    "    nums_fisher_examples = torch.Tensor([(len(client_loader)-1)*client_loader.batch_size for client_loader in client_loaders])\n",
    "    nums_regmean_examples = nums_fisher_examples.clone()\n",
    "    clients_this_round = server_batch  # record the number of clients for this round\n",
    "\n",
    "    for i, client_id in enumerate(client_ids):\n",
    "        # Download Model\n",
    "        client_model = client_models[i]\n",
    "        client_model.to(device)\n",
    "\n",
    "        # Local Training\n",
    "        # client_opt = torch.optim.SGD(client_model.parameters(), lr=client_lr, momentum=0.9)\n",
    "        client_opt = torch.optim.Adam(client_model.parameters(), lr=client_lr)\n",
    "        client_loader = clients[client_id]\n",
    "        client_acc = {}\n",
    "        \n",
    "        for epoch in range(client_epochs):\n",
    "            for x,y in client_loader:\n",
    "                loss, stats = test_batch(client_model, x, y)\n",
    "                \n",
    "                client_opt.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                if l2_clip_norm > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(client_model.parameters(), l2_clip_norm)\n",
    "\n",
    "                client_opt.step()\n",
    "\n",
    "                for k,v in stats.items():\n",
    "                    client_acc[k] = client_acc.get(k, 0) + v\n",
    "                pbar.set_description(f\"eval: {eval_accu} | client {i}, epoch {epoch} | loss {loss:.4f}\")\n",
    "                \n",
    "            if epoch % 1 == 0:\n",
    "                eval_model = deepcopy(client_model)\n",
    "                eval_model.to(device)\n",
    "\n",
    "                eval_results = eval_loop(eval_model, client_loader)\n",
    "                print(\"Accuracy is {}\".format(get_metric(eval_results, \"accu\")))\n",
    "\n",
    "        # This is our delta parameter\n",
    "        client_model.to(\"cpu\")  # move to cpu to save memory\n",
    "        neg_client_delta = {\n",
    "            n: server_params[n].data - cp.data for n,cp \n",
    "                            in client_model.named_parameters() if cp.requires_grad\n",
    "        }\n",
    "        neg_client_deltas.append(neg_client_delta)\n",
    "\n",
    "        # Log last iteration\n",
    "        client_acc['norm'] = 0\n",
    "        for k,v in client_acc.items():\n",
    "            stats_acc[k] = stats_acc.get(k, 0) + v\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the performances of the finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model = deepcopy(client_models[1])\n",
    "eval_model.to(device)\n",
    "\n",
    "eval_results = eval_loop(eval_model, clients[1])\n",
    "\n",
    "get_metric(eval_results, \"accu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model = deepcopy(client_models[0])\n",
    "eval_model.to(device)\n",
    "\n",
    "eval_results = eval_loop(eval_model, clients[0])\n",
    "\n",
    "get_metric(eval_results, \"accu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merging\n",
    "\n",
    "need to run them all everytime we try a new merging method, because we edit the `cur_server_model` in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the pretraiend model\n",
    "cur_server_model = deepcopy(orig_server_model)\n",
    "cur_server_params = {n:p for n,p in cur_server_model.named_parameters() if p.requires_grad}\n",
    "cur_server_opt = torch.optim.SGD(cur_server_params.values(), lr=server_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for merging\n",
    "merger = MergingFactory.get_merging_strategy(\"average\", cur_server_model, args=args)  # average, task_arithmetic, fisher_merging, ties_merging, regmean_merging\n",
    "scaling_coefficient = 1.  # controls how far we want to go in the direction of the aggregated task vector, only used in task arithmetic\n",
    "param_value_mask_rate = 0.5  # controls how much parameters we want to prune/drop in ties-merging (and dare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging\n",
    "aggregated_update = merger.aggregate_updates(neg_client_deltas,\n",
    "                                                sample_num_list=sample_num_list,\n",
    "                                                scaling_coefficient=scaling_coefficient,\n",
    "                                                client_loaders=client_loaders,\n",
    "                                                test_batch=test_batch,\n",
    "                                                nums_fisher_examples=nums_fisher_examples,\n",
    "                                                nums_regmean_examples=nums_regmean_examples,\n",
    "                                                device=device,\n",
    "                                                normalize_fisher_weight=True,\n",
    "                                                minimal_fisher_weight = 1e-6,\n",
    "                                                param_value_mask_rate= param_value_mask_rate)\n",
    "merger.update_server_model(aggregated_update, cur_server_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.379746835443038\n",
      "0.005813953488372093\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "eval_model = deepcopy(cur_server_model)\n",
    "eval_model.to(device)\n",
    "\n",
    "eval_results = eval_loop(eval_model, clients[0])\n",
    "print(get_metric(eval_results, \"accu\"))\n",
    "\n",
    "eval_results = eval_loop(eval_model, clients[1])\n",
    "print(get_metric(eval_results, \"accu\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flasc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
